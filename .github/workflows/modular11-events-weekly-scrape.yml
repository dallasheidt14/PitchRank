name: Modular11 Events Weekly Scrape

on:
  # Schedule disabled - manual runs only
  # schedule:
  #   # Run every Sunday at 11pm Mountain Time
  #   # MT is UTC-6 (MDT) or UTC-7 (MST)
  #   # Sunday 11pm MDT = Monday 5am UTC
  #   # Sunday 11pm MST = Monday 6am UTC
  #   # Using Monday 5am UTC to cover most of the year (daylight time)
  #   # Cron format: minute hour day-of-month month day-of-week
  #   # 0 = Sunday, 1 = Monday
  #   - cron: '0 5 * * 1'  # Every Monday at 5am UTC (Sunday 11pm MT)
  workflow_dispatch:
    inputs:
      age_min:
        description: 'Minimum age group (e.g. 13 for U13)'
        required: false
        default: '13'
      age_max:
        description: 'Maximum age group (e.g. 17 for U17)'
        required: false
        default: '17'
      days_back:
        description: 'Number of days back to scrape'
        required: false
        default: '21'
      dry_run:
        description: 'Dry run (scrape only, no import)'
        type: boolean
        required: false
        default: false

jobs:
  scrape-and-import:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hour timeout

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Install Scrapy if not in requirements.txt
          pip install scrapy twisted
          mkdir -p logs scrapers/modular11_scraper/output

      - name: Run Modular11 Events Scraper
        env:
          PYTHONPATH: ${{ github.workspace }}:${{ github.workspace }}/scrapers/modular11_scraper
        run: |
          cd scrapers/modular11_scraper
          
          # Verify we're in the right directory (should have scrapy.cfg)
          if [ ! -f "scrapy.cfg" ]; then
            echo "ERROR: scrapy.cfg not found. Current directory: $(pwd)"
            ls -la
            exit 1
          fi
          
          AGE_MIN="${{ github.event.inputs.age_min || '13' }}"
          AGE_MAX="${{ github.event.inputs.age_max || '17' }}"
          DAYS_BACK="${{ github.event.inputs.days_back || '21' }}"
          
          echo "Scraping events: U${AGE_MIN}-U${AGE_MAX}, last ${DAYS_BACK} days"
          
          # Scrape events for configured age range and days back
          python -m scrapy crawl modular11_events \
            -a age_min=$AGE_MIN \
            -a age_max=$AGE_MAX \
            -a days_back=$DAYS_BACK
          
          # Find the most recent CSV file
          if [ ! -d "output" ]; then
            echo "ERROR: output directory not found"
            exit 1
          fi
          
          cd output
          LATEST_CSV=$(ls -t modular11_results_*.csv 2>/dev/null | head -n 1)
          
          if [ -z "$LATEST_CSV" ]; then
            echo "ERROR: No CSV file generated"
            ls -la || echo "Output directory contents:"
            exit 1
          fi
          
          ROW_COUNT=$(wc -l < "$LATEST_CSV")
          echo "Scraped file: $LATEST_CSV ($ROW_COUNT rows)"
          
          # Get absolute path from workspace root
          CSV_PATH="${{ github.workspace }}/scrapers/modular11_scraper/output/$LATEST_CSV"
          echo "CSV_FILE=$CSV_PATH" >> $GITHUB_ENV
          echo "ROW_COUNT=$ROW_COUNT" >> $GITHUB_ENV

      - name: Import Games
        if: ${{ github.event.inputs.dry_run != 'true' }}
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          export PYTHONPATH="${PYTHONPATH}:${PWD}"
          cd ${{ github.workspace }}
          
          if [ -z "$CSV_FILE" ]; then
            echo "ERROR: CSV_FILE not set"
            exit 1
          fi
          
          echo "==========================================="
          echo "IMPORTING: $CSV_FILE ($ROW_COUNT rows)"
          echo "==========================================="
          
          # Run import with --batch-size 500 for Supabase reliability
          # --summary-only to reduce log noise
          python scripts/import_games_enhanced.py "$CSV_FILE" modular11 \
            --batch-size 500 \
            --summary-only

      - name: Post-Import Duplicate Safety Check
        if: ${{ github.event.inputs.dry_run != 'true' }}
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          export PYTHONPATH="${PYTHONPATH}:${PWD}"
          cd ${{ github.workspace }}
          
          echo "==========================================="
          echo "POST-IMPORT: Checking for duplicate games"
          echo "==========================================="
          
          # Dry-run the composite-key duplicate checker
          # If duplicates are found, this will report them (but NOT delete)
          python scripts/cleanup_dupe_games_by_composite.py

      - name: Upload scraped data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: modular11-events-games-${{ github.run_number }}
          path: scrapers/modular11_scraper/output/modular11_results_*.csv
          retention-days: 30
          if-no-files-found: ignore

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: modular11-events-logs-${{ github.run_number }}
          path: |
            logs/*.log
            scrapers/modular11_scraper/*.log
          retention-days: 30
          if-no-files-found: ignore

      - name: Report results
        if: always()
        run: |
          echo "===================="
          echo "Modular11 Events Weekly Scrape completed"
          echo "===================="
          echo "Date range: Last ${{ github.event.inputs.days_back || '21' }} days"
          echo "Age groups: U${{ github.event.inputs.age_min || '13' }}-U${{ github.event.inputs.age_max || '17' }}"
          echo "Source: MLS NEXT Events/Showcases"
          echo "Dry run: ${{ github.event.inputs.dry_run || 'false' }}"
          echo "Check artifacts for scraped data and logs"
