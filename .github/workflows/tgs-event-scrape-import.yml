name: TGS Event Scrape and Import

on:
  workflow_dispatch:
    inputs:
      start_event:
        description: 'Starting event ID (e.g., 3900)'
        required: true
        type: string
      end_event:
        description: 'Ending event ID (e.g., 3950)'
        required: true
        type: string
      skip_import:
        description: 'Skip import step (only scrape)'
        required: false
        type: boolean
        default: false

jobs:
  scrape-and-import:
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4 hour timeout (scraping and importing can take a while)
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create data directories
        run: |
          mkdir -p data/raw/tgs
          mkdir -p data/cache
          mkdir -p logs
      
      - name: Scrape TGS Events
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          export PYTHONPATH="${PYTHONPATH}:${PWD}"
          echo "Scraping TGS events ${{ inputs.start_event }} through ${{ inputs.end_event }}"
          python scripts/scrape_tgs_event.py --start-event ${{ inputs.start_event }} --end-event ${{ inputs.end_event }}
      
      - name: Find scraped CSV file
        id: find_csv
        run: |
          # Find the most recent CSV file matching the pattern
          CSV_FILE=$(ls -t data/raw/tgs/tgs_events_${{ inputs.start_event }}_${{ inputs.end_event }}_*.csv 2>/dev/null | head -n 1)
          if [ -z "$CSV_FILE" ]; then
            echo "Error: Could not find scraped CSV file"
            exit 1
          fi
          echo "Found CSV file: $CSV_FILE"
          echo "csv_file=$CSV_FILE" >> $GITHUB_OUTPUT
      
      - name: Import Games
        if: inputs.skip_import == false
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          export PYTHONPATH="${PYTHONPATH}:${PWD}"
          CSV_FILE="${{ steps.find_csv.outputs.csv_file }}"
          echo "Importing games from: $CSV_FILE"
          python scripts/import_games_enhanced.py "$CSV_FILE" tgs
      
      - name: Upload scraped data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: tgs-events-${{ inputs.start_event }}-${{ inputs.end_event }}-${{ github.run_number }}
          path: |
            data/raw/tgs/tgs_events_${{ inputs.start_event }}_${{ inputs.end_event }}_*.csv
          retention-days: 30
          if-no-files-found: warn
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: tgs-logs-${{ inputs.start_event }}-${{ inputs.end_event }}-${{ github.run_number }}
          path: logs/*.log
          retention-days: 30
          if-no-files-found: ignore
      
      - name: Report results
        if: always()
        run: |
          echo "TGS event scrape and import workflow completed"
          echo "Events: ${{ inputs.start_event }} through ${{ inputs.end_event }}"
          if [ "${{ inputs.skip_import }}" == "false" ]; then
            echo "Import: Completed"
          else
            echo "Import: Skipped"
          fi
          echo "Check the artifacts above for scraped data and logs."


