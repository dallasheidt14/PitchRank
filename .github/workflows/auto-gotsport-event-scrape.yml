name: Auto GotSport Event Scrape

on:
  schedule:
    # Run every Sunday night at 11pm Mountain Time
    # GitHub Actions cron uses UTC timezone
    # MST (UTC-7): 11:00 PM MST Sunday = 6:00 AM UTC Monday
    # MDT (UTC-6): 11:00 PM MDT Sunday = 5:00 AM UTC Monday
    # Using 6:00 AM UTC Monday means:
    #   - During MST: runs at 11:00 PM Sunday ✅
    #   - During MDT: runs at 12:00 AM Monday (1 hour later, acceptable)
    - cron: '0 6 * * 1'  # Every Monday at 6:00 AM UTC (≈11:00 PM Mountain Time Sunday)
  workflow_dispatch: # Allow manual trigger from GitHub Actions tab

jobs:
  scrape-events:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout (scraping can take a while)
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Additional dependencies for scraping
          pip install beautifulsoup4 lxml requests certifi
      
      - name: Create data directories
        run: |
          mkdir -p data/raw
          mkdir -p data/cache
          mkdir -p logs
      
      - name: Run GotSport Event Scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          PYTHONPATH: ${{ github.workspace }}
          # Aggressive performance settings to prevent timeout
          GOTSPORT_DELAY_MIN: '0.1'
          GOTSPORT_DELAY_MAX: '0.3'
          GOTSPORT_MAX_RETRIES: '2'
          GOTSPORT_TIMEOUT: '10'
          GOTSPORT_MAX_SCHEDULE_PAGES: '10'
          # Per-event timeout in seconds (skip events taking too long)
          GOTSPORT_EVENT_TIMEOUT: '180'
        run: |
          # Set Python path to include project root
          export PYTHONPATH="${PYTHONPATH}:${PWD}"
          # Run the event scraper with aggressive timeout protection
          # --max-runtime 90: Stop after 1.5 hours (leave 1.5h buffer for 3h limit)
          # --max-events 20: Limit to 20 events per run to prevent timeout
          # --days-back 5: Reduce search range from 7 to 5 days
          # Auto-import is enabled by default
          python scripts/scrape_new_gotsport_events.py --max-runtime 90 --max-events 20 --days-back 5 || exit 1
      
      - name: Upload scraped data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gotsport-events-${{ github.run_number }}
          path: |
            data/raw/new_events_*.jsonl
            data/raw/new_events_*_summary.json
            data/raw/scraped_events.json
          retention-days: 30
          if-no-files-found: warn
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gotsport-events-logs-${{ github.run_number }}
          path: logs/*.log
          retention-days: 30
          if-no-files-found: ignore
      
      - name: Report results
        if: always()
        run: |
          echo "GotSport event scraper workflow completed."
          echo "Check the artifacts above for scraped data and logs."

