name: Auto GotSport Event Scrape

on:
  schedule:
    # Run twice weekly to catch more events with smaller batches
    # Sunday night + Wednesday night Mountain Time
    # GitHub Actions cron uses UTC timezone
    - cron: '0 6 * * 1'  # Monday 6:00 AM UTC (≈11:00 PM Sunday MT)
    - cron: '0 6 * * 4'  # Thursday 6:00 AM UTC (≈11:00 PM Wednesday MT)
  workflow_dispatch: # Allow manual trigger from GitHub Actions tab

jobs:
  scrape-events:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout (scraping can take a while)
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Additional dependencies for scraping
          pip install beautifulsoup4 lxml requests certifi
      
      - name: Create data directories
        run: |
          mkdir -p data/raw
          mkdir -p data/cache
          mkdir -p logs
      
      - name: Run GotSport Event Scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          PYTHONPATH: ${{ github.workspace }}
          # Performance settings - balanced for speed + completeness
          GOTSPORT_DELAY_MIN: '0.1'
          GOTSPORT_DELAY_MAX: '0.2'
          GOTSPORT_MAX_RETRIES: '2'
          GOTSPORT_TIMEOUT: '12'
          # Keep schedule pages high enough to capture all brackets
          GOTSPORT_MAX_SCHEDULE_PAGES: '25'
          GOTSPORT_PAGE_DELAY: '0.05'
          # Per-event timeout: skip events taking >4 min (likely stuck)
          GOTSPORT_EVENT_TIMEOUT: '240'
        run: |
          # Set Python path to include project root
          export PYTHONPATH="${PYTHONPATH}:${PWD}"
          # Run the event scraper with timeout protection
          # --max-runtime 100: Stop after 100 min (leave 80 min buffer for 3h limit)
          # --max-events 25: Process up to 25 events per run
          # --days-back 7: Full week lookback for event discovery
          # The key fix is early-exit from discovery once max_events found
          python scripts/scrape_new_gotsport_events.py --max-runtime 100 --max-events 25 --days-back 7 || exit 1
      
      - name: Upload scraped data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gotsport-events-${{ github.run_number }}
          path: |
            data/raw/new_events_*.jsonl
            data/raw/new_events_*_summary.json
            data/raw/scraped_events.json
          retention-days: 30
          if-no-files-found: warn
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gotsport-events-logs-${{ github.run_number }}
          path: logs/*.log
          retention-days: 30
          if-no-files-found: ignore
      
      - name: Report results
        if: always()
        run: |
          echo "GotSport event scraper workflow completed."
          echo "Check the artifacts above for scraped data and logs."

